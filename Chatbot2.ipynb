{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18de3f6a-cab6-4373-831f-5bf123c99550",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "731c7e65-b5df-4d7f-a4aa-05c177e28068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d64492-9ab7-4c8a-ab43-e651aa3ca2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745b5610-6b60-4c2a-847a-eea8597ddb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your own training data from a CSV file\n",
    "train_data = pd.read_csv('faq_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99f811e-961b-420e-a948-b6c7f6e84884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I locate my card?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I still have not received my new card, I order...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ordered a card but it has not arrived. Help ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is there a way to know when my card will arrive?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My card has not arrived yet.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  category\n",
       "0                           How do I locate my card?        11\n",
       "1  I still have not received my new card, I order...        11\n",
       "2  I ordered a card but it has not arrived. Help ...        11\n",
       "3   Is there a way to know when my card will arrive?        11\n",
       "4                       My card has not arrived yet.        11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab733da-638a-41dc-b5bb-003955b27179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FAQ categories\n",
    "faq_categories = {\n",
    "    0: 'card arrival',\n",
    "    1: 'card linking',\n",
    "    2: 'exchange rate',\n",
    "    3: 'card payment wrong exchange rate',\n",
    "    4: 'extra charge on statement',\n",
    "    5: 'pending cash withdrawal',\n",
    "    6: 'fiat currency support',\n",
    "    7: 'card delivery estimate',\n",
    "    8: 'automatic top up',\n",
    "    9: 'card not working',\n",
    "    10: 'exchange via app',\n",
    "    11: 'lost or stolen card',\n",
    "    12: 'age limit',\n",
    "    13: 'pin blocked',\n",
    "    14: 'contactless not working',\n",
    "    15: 'top up by bank transfer charge',\n",
    "    16: 'pending top up',\n",
    "    17: 'cancel transfer',\n",
    "    18: 'top up limits',\n",
    "    19: 'wrong amount of cash received',\n",
    "    20: 'card payment fee charged',\n",
    "    21: 'transfer not received by recipient',\n",
    "    22: 'supported cards and currencies',\n",
    "    23: 'getting virtual card',\n",
    "    24: 'card acceptance',\n",
    "    25: 'top up reverted',\n",
    "    26: 'balance not updated after cheque or cash deposit',\n",
    "    27: 'card payment not recognised',\n",
    "    28: 'edit personal details',\n",
    "    29: 'why verify identity',\n",
    "    30: 'unable to verify identity',\n",
    "    31: 'get physical card',\n",
    "    32: 'visa or mastercard',\n",
    "    33: 'topping up by card',\n",
    "    34: 'disposable card limits',\n",
    "    35: 'compromised card',\n",
    "    36: 'atm support',\n",
    "    37: 'direct debit payment not recognised',\n",
    "    38: 'passcode forgotten',\n",
    "    39: 'declined cash withdrawal',\n",
    "    40: 'pending card payment',\n",
    "    41: 'lost or stolen phone',\n",
    "    42: 'request refund',\n",
    "    43: 'declined transfer',\n",
    "    44: 'Refund not showing up',\n",
    "    45: 'declined card payment',\n",
    "    46: 'pending transfer',\n",
    "    47: 'terminate account',\n",
    "    48: 'card swallowed',\n",
    "    49: 'transaction charged twice',\n",
    "    50: 'verify source of funds',\n",
    "    51: 'transfer timing',\n",
    "    52: 'reverted card payment?',\n",
    "    53: 'change pin',\n",
    "    54: 'beneficiary not allowed',\n",
    "    55: 'transfer fee charged',\n",
    "    56: 'receiving money',\n",
    "    57: 'failed transfer',\n",
    "    58: 'transfer into account',\n",
    "    59: 'verify top up',\n",
    "    60: 'getting spare card',\n",
    "    61: 'top up by cash or cheque',\n",
    "    62: 'order physical card',\n",
    "    63: 'virtual card not working',\n",
    "    64: 'wrong exchange rate for cash withdrawal',\n",
    "    65: 'get disposable virtual card',\n",
    "    66: 'top up failed',\n",
    "    67: 'balance not updated after bank transfer',\n",
    "    68: 'cash withdrawal not recognised',\n",
    "    69: 'exchange charge',\n",
    "    70: 'top up by card charge',\n",
    "    71: 'activate my card',\n",
    "    72: 'cash withdrawal charge',\n",
    "    73: 'card about to expire',\n",
    "    74: 'apple pay or google pay',\n",
    "    75: 'verify my identity',\n",
    "    76: 'country support',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8765711-4153-4eb4-a455-f873f7fc9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained spaCy model for entity recognition\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95446d10-403f-413e-bb4b-41a81b14ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained language model\n",
    "language_model = \"gpt2\"  # or other language models like \"gpt2-medium\", \"gpt2-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7fa6cf8-41b8-4cbf-ab2e-3ef21bdc38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities using spaCy\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11093545-822d-48a0-bb82-1b8e08f5f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "def train_classifier():\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Load your own training data from a CSV file\n",
    "    train_data = pd.read_csv('faq_data.csv')\n",
    "    X_train = vectorizer.fit_transform(train_data['question'])\n",
    "    y_train = train_data['category']\n",
    "    \n",
    "    # Train a LinearSVC classifier\n",
    "    classifier = LinearSVC() #classifier = SVC()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    return vectorizer, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e20c1-359e-4feb-b323-c23dbd33b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0618ed-3eb1-4ebb-a367-aca0138948d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cc42a-7e59-4031-b584-8ad7ac8e79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the best classifier\n",
    "predictions = best_classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8eb529-cb75-4548-b926-51685d22d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify customer questions using the trained model\n",
    "def classify_question(question, vectorizer, classifier):\n",
    "    X = vectorizer.transform([question])\n",
    "    \n",
    "    # Classify the customer question\n",
    "    category_id = classifier.predict(X)[0]\n",
    "    category = faq_categories[category_id]\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "007d1c24-a42a-45d2-bd8f-38dca12fbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier and entity recognition\n",
    "vectorizer, classifier = train_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60012ab7-8301-46ba-a3dc-42cf32b1459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello. How can I help you?\n",
      "***Enter bye/end/quit to end the chat***\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Remove the \"User:\" prefix from the input\u001b[39;00m\n\u001b[0;32m     13\u001b[0m user_input \u001b[38;5;241m=\u001b[39m user_input\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 15\u001b[0m category \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m entities \u001b[38;5;241m=\u001b[39m extract_entities(user_input)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, category)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mclassify_question\u001b[1;34m(question, vectorizer, classifier)\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([question])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Classify the customer question\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m category_id \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m category \u001b[38;5;241m=\u001b[39m faq_categories[category_id]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m category\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:881\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1114\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         )\n\u001b[0;32m   1118\u001b[0m     )\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1126\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1126\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1127\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1128\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:179\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[1;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    178\u001b[0m     )\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2561\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2560\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2561\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2619\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2616\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2622\u001b[0m     )\n\u001b[0;32m   2624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2628\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "print(\"Chatbot: Hello. How can I help you?\\n***Enter bye/end/quit to end the chat***\") \n",
    "while True:\n",
    "    \n",
    "    # Get user input\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    # Check if user wants to end the conversation\n",
    "    if user_input.lower() in [\"bye\", \"end\", \"quit\"]:\n",
    "        print(\"ChatBot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Remove the \"User:\" prefix from the input\n",
    "    user_input = user_input.replace(\"User: \", \"\").strip()\n",
    "    \n",
    "    category = classify_question(user_input, vectorizer, classifier)\n",
    "    entities = extract_entities(user_input)\n",
    "    \n",
    "    print(\"Category:\", category)\n",
    "    print(\"Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d49c94-2643-47a3-a2ee-87e79fdccf4b",
   "metadata": {},
   "source": [
    "GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b6664aa-8a43-4d6f-99ac-fc07e2520f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e0cf36-62ff-4e83-b84d-90a0f6c16b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8281b0533d4844428a859b34c1df78b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahap\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sahap\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876e49bc67e145e4a59cb1ff73babb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e309a127bb6c4675855ea170adfb6f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download and load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2881b79f-0fa2-4b09-9298-e8433c71423d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b885defbd703473db09928bd15d1398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4f5d95123c4e92bb012dd1a5590585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download and load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aede1585-5f85-4c39-a248-0ee1eac27199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73f56c0f-de3f-4e9b-94a2-12bbdf12205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the generation parameters\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\", config=model_config)\n",
    "model_config.pad_token_id = model_config.eos_token_id\n",
    "\n",
    "# Encode input text\n",
    "input_text = \"Hey how are you? i need some help changing the password to my bank account\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71e00bfe-6b65-4f1c-968b-7f44ebddf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output using the model\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    #config=model_config,\n",
    "    num_return_sequences=1,  # Number of different sequences to generate\n",
    "    do_sample=True,  # Enable random sampling\n",
    "    temperature=0.7,  # Controls randomness, higher value -> more random\n",
    "    repetition_penalty=1.0,  # Controls repetition, higher value -> less repetition\n",
    "    length_penalty=1.0,  # Controls output length, lower value -> shorter length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17cccf36-fa32-46c8-8b1a-25b2035accd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey how are you? i need some help changing the password to my bank account now. i need this to be saved. so i can pay for this. i need to get my car back to the dealership. i need the car keys to the dealership.Give me this and i'll pay for the car keys.I can use this to get my car back to the dealership.I need to get my car back to the dealership.\n",
      "\n",
      "Anonymous 02/22/16 (Thu) 07\n"
     ]
    }
   ],
   "source": [
    "# Decode and print the generated response\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970e063-e964-4bc3-b264-6dcdbcfae423",
   "metadata": {},
   "source": [
    "Save GP2 and run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "527433d3-7951-432f-b6b8-27e52f08707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce7f565c-abf9-41e3-baf6-14651f8b0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Download and load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Download and load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Download and load the GPT-2 model\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Save the tokenizer and model to your local computer\n",
    "tokenizer.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/tokenizer\")\n",
    "model.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/model\")\n",
    "model_config.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/model_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b8e8a8f-4208-4f1a-a919-f7f40aaeff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer and model from the local saved files\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/tokenizer\")\n",
    "model_config = GPT2Config.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/model_config\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/model\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62e5f1c7-19ab-4c4c-b596-6be8b73fa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the attention mask and pad token ID\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ec4c878-f589-466f-94c1-f11c64fe89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  i want to open a new account but I am unable to contact my account banker.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:  summarize: \n",
      "I have been told that the bank will not be able, and they are going through with it because there is no way of getting an answer from them about what happened in this case (i think). So if you would like me on your side please send us some information so we can find out more details as soon possible!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    # Check if user wants to end the conversation\n",
    "    if user_input.lower() in [\"bye\", \"end\", \"quit\"]:\n",
    "        print(\"ChatBot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Remove the \"User:\" prefix from the input\n",
    "    user_input = user_input.replace(\"User: \", \"\").strip()\n",
    "    \n",
    "    # Tokenize the user input\n",
    "    input_ids = tokenizer.encode(\"\"+user_input, return_tensors=\"pt\")\n",
    "\n",
    "    # Set the attention mask\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    # Generate response using the model\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,  # Number of different sequences to generate\n",
    "        do_sample=True,  # Enable random sampling\n",
    "        temperature=0.2,  # Controls randomness, higher value -> more random 0.7\n",
    "        repetition_penalty=7.0,  # Controls repetition, higher value -> less repetition\n",
    "        length_penalty=0.5,  # Controls output length, lower value -> shorter length\n",
    "    )\n",
    "\n",
    "    # Convert the output ids to text\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove the input text from the generated response\n",
    "    response = response.replace(user_input, \"\").strip()\n",
    "\n",
    "    print(\"ChatBot: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65412002-3fb5-44d7-b40d-ab64f1c1c6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69b67698-16f1-431b-9f8e-73df2a7525fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum length for generated responses\n",
    "max_response_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "242d6b8c-1ae0-4bfd-8759-f89fe6957e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customize the generation parameters\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\", config=model_config)\n",
    "model_config.pad_token_id = model_config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d638f2d4-474f-478c-93fa-ded887e1c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hello how are you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot: hello how are you today?\n",
      "\n",
      "I'm sorry, but my first love was to my sister.\n",
      "\n",
      "What do you do for a living?\n",
      "\n",
      "I live as a student.\n",
      "\n",
      "My parents died.\n",
      "\n",
      "Did you go to university?\n",
      "\n",
      "Yes.\n",
      "\n",
      "You have an amazing job?rafted in?\n",
      "\n",
      "Yes.\n",
      "\n",
      "You don't work at a big corporate organization?\n",
      "\n",
      "No, I don't work at a big corporate organization.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Tokenize the user input\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(user_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # Tokenize the user input\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response using the model\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_response_length,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "    )\n",
    "\n",
    "    # Convert the output ids to text\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"ChatBot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af4d762a-de19-4efb-9e0c-8bf8a58f08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c1c91fe0-2f6d-4c24-8fb8-dc513e9bd425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d09e558f3d483dbd27ebfa51d1c901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf033102706422dad1f3c0db305cb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf2f4ab122b4c5fa78508b67e761d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1176274bc533402799a3284a63d13794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996980428695679}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51db8e05-4a6e-49e4-a5d9-cfef7a3dec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9994600415229797}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('This is ridiculous. This is the fourth time of me trying to call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986adab-601c-43b1-9c47-ecfdbd85f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Save DistillBert and run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "215cad66-9093-4776-b204-27819ff26cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bcea4d0-b945-443c-a448-11f16c48e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'  # or other DistilBERT variants\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "classification_model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5aa3e7ba-a1a9-4560-a9e0-d796df9f73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and model to your local computer\n",
    "tokenizer.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/tokenizer\")\n",
    "classification_model.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d142a21a-80fc-4685-9880-3b7cf94debe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model from the local saved files\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/tokenizer\")\n",
    "classification_model = DistilBertForSequenceClassification.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/model\", num_labels=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ef6b5bbc-0216-461e-a7ef-77cb24d9d99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8972105464272068\n",
      "Recall: 0.8972105464272068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('faq_data.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['question'], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ed4617f-8753-4e10-954e-4c5b70ff48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1}\n",
      "Accuracy: 0.8972105464272068\n",
      "Recall: 0.8972105464272068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Create a parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "\n",
    "# Create a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best classifier\n",
    "predictions = best_classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae99d39-c6eb-4d8e-9b6d-2411465e42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('faq_data.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['question'], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Create an SVM classifier\n",
    "classifier = SVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on user input\n",
    "while True:\n",
    "    user_input = input(\"Enter your question (or 'q' to quit): \")\n",
    "    if user_input == 'q':\n",
    "        break\n",
    "\n",
    "    # Transform user input into vector using the same vectorizer\n",
    "    user_input_vector = vectorizer.transform([user_input])\n",
    "\n",
    "    # Predict the category of the user input\n",
    "    prediction = best_classifier.predict(user_input_vector)[0]\n",
    "\n",
    "    # Frame the category as a sentence\n",
    "    category_sentence = f\"The category of your question is: {prediction}\"\n",
    "\n",
    "    print(category_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437a499-63ee-4820-8c63-a1c0300821f5",
   "metadata": {},
   "source": [
    "################# FINAl CATEGORY AND NER ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1f875-1623-47a4-9fac-bf11e630085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('faq_data.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['question'], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "\n",
    "# Create a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Load Stanford NER model\n",
    "stanford_ner_model = 'path_to_stanford_ner_model'\n",
    "stanford_ner_jar = 'path_to_stanford_ner_jar'\n",
    "\n",
    "# Create NER tagger\n",
    "ner_tagger = StanfordNERTagger(stanford_ner_model, stanford_ner_jar, encoding='utf-8')\n",
    "\n",
    "# Function to extract entities from the question using NER\n",
    "def extract_entities(question):\n",
    "    tokenized_question = word_tokenize(question)\n",
    "    entities = ner_tagger.tag(tokenized_question)\n",
    "    return [entity for entity, tag in entities if tag != 'O']\n",
    "\n",
    "# Function to classify user input and frame the response\n",
    "def classify_question(user_input):\n",
    "    user_input_vector = vectorizer.transform([user_input])\n",
    "    predicted_category = best_classifier.predict(user_input_vector)[0]\n",
    "    entities = extract_entities(user_input)\n",
    "    response = f\"The category of your question is {predicted_category}. The entities mentioned are: {', '.join(entities)}.\"\n",
    "    return response\n",
    "\n",
    "# Chatbot loop\n",
    "while True:\n",
    "    user_input = input(\"Please enter your question (or 'q' to quit): \")\n",
    "    if user_input.lower() == 'q':\n",
    "        break\n",
    "    else:\n",
    "        response = classify_question(user_input)\n",
    "        print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
