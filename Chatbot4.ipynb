{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18de3f6a-cab6-4373-831f-5bf123c99550",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "731c7e65-b5df-4d7f-a4aa-05c177e28068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d64492-9ab7-4c8a-ab43-e651aa3ca2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745b5610-6b60-4c2a-847a-eea8597ddb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your own training data from a CSV file\n",
    "train_data = pd.read_csv('faq_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99f811e-961b-420e-a948-b6c7f6e84884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I locate my card?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I still have not received my new card, I order...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ordered a card but it has not arrived. Help ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is there a way to know when my card will arrive?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My card has not arrived yet.</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  category\n",
       "0                           How do I locate my card?        11\n",
       "1  I still have not received my new card, I order...        11\n",
       "2  I ordered a card but it has not arrived. Help ...        11\n",
       "3   Is there a way to know when my card will arrive?        11\n",
       "4                       My card has not arrived yet.        11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab733da-638a-41dc-b5bb-003955b27179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FAQ categories\n",
    "faq_categories = {\n",
    "    0: 'card arrival',\n",
    "    1: 'card linking',\n",
    "    2: 'exchange rate',\n",
    "    3: 'card payment wrong exchange rate',\n",
    "    4: 'extra charge on statement',\n",
    "    5: 'pending cash withdrawal',\n",
    "    6: 'fiat currency support',\n",
    "    7: 'card delivery estimate',\n",
    "    8: 'automatic top up',\n",
    "    9: 'card not working',\n",
    "    10: 'exchange via app',\n",
    "    11: 'lost or stolen card',\n",
    "    12: 'age limit',\n",
    "    13: 'pin blocked',\n",
    "    14: 'contactless not working',\n",
    "    15: 'top up by bank transfer charge',\n",
    "    16: 'pending top up',\n",
    "    17: 'cancel transfer',\n",
    "    18: 'top up limits',\n",
    "    19: 'wrong amount of cash received',\n",
    "    20: 'card payment fee charged',\n",
    "    21: 'transfer not received by recipient',\n",
    "    22: 'supported cards and currencies',\n",
    "    23: 'getting virtual card',\n",
    "    24: 'card acceptance',\n",
    "    25: 'top up reverted',\n",
    "    26: 'balance not updated after cheque or cash deposit',\n",
    "    27: 'card payment not recognised',\n",
    "    28: 'edit personal details',\n",
    "    29: 'why verify identity',\n",
    "    30: 'unable to verify identity',\n",
    "    31: 'get physical card',\n",
    "    32: 'visa or mastercard',\n",
    "    33: 'topping up by card',\n",
    "    34: 'disposable card limits',\n",
    "    35: 'compromised card',\n",
    "    36: 'atm support',\n",
    "    37: 'direct debit payment not recognised',\n",
    "    38: 'passcode forgotten',\n",
    "    39: 'declined cash withdrawal',\n",
    "    40: 'pending card payment',\n",
    "    41: 'lost or stolen phone',\n",
    "    42: 'request refund',\n",
    "    43: 'declined transfer',\n",
    "    44: 'Refund not showing up',\n",
    "    45: 'declined card payment',\n",
    "    46: 'pending transfer',\n",
    "    47: 'terminate account',\n",
    "    48: 'card swallowed',\n",
    "    49: 'transaction charged twice',\n",
    "    50: 'verify source of funds',\n",
    "    51: 'transfer timing',\n",
    "    52: 'reverted card payment?',\n",
    "    53: 'change pin',\n",
    "    54: 'beneficiary not allowed',\n",
    "    55: 'transfer fee charged',\n",
    "    56: 'receiving money',\n",
    "    57: 'failed transfer',\n",
    "    58: 'transfer into account',\n",
    "    59: 'verify top up',\n",
    "    60: 'getting spare card',\n",
    "    61: 'top up by cash or cheque',\n",
    "    62: 'order physical card',\n",
    "    63: 'virtual card not working',\n",
    "    64: 'wrong exchange rate for cash withdrawal',\n",
    "    65: 'get disposable virtual card',\n",
    "    66: 'top up failed',\n",
    "    67: 'balance not updated after bank transfer',\n",
    "    68: 'cash withdrawal not recognised',\n",
    "    69: 'exchange charge',\n",
    "    70: 'top up by card charge',\n",
    "    71: 'activate my card',\n",
    "    72: 'cash withdrawal charge',\n",
    "    73: 'card about to expire',\n",
    "    74: 'apple pay or google pay',\n",
    "    75: 'verify my identity',\n",
    "    76: 'country support',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8765711-4153-4eb4-a455-f873f7fc9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained spaCy model for entity recognition\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95446d10-403f-413e-bb4b-41a81b14ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained language model\n",
    "language_model = \"gpt2\"  # or other language models like \"gpt2-medium\", \"gpt2-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7fa6cf8-41b8-4cbf-ab2e-3ef21bdc38ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract entities using spaCy\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11093545-822d-48a0-bb82-1b8e08f5f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "def train_classifier():\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Load your own training data from a CSV file\n",
    "    train_data = pd.read_csv('faq_data.csv')\n",
    "    X_train = vectorizer.fit_transform(train_data['question'])\n",
    "    y_train = train_data['category']\n",
    "    \n",
    "    # Train a LinearSVC classifier\n",
    "    classifier = LinearSVC() #classifier = SVC()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    return vectorizer, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e20c1-359e-4feb-b323-c23dbd33b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0618ed-3eb1-4ebb-a367-aca0138948d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cc42a-7e59-4031-b584-8ad7ac8e79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the best classifier\n",
    "predictions = best_classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f8eb529-cb75-4548-b926-51685d22d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify customer questions using the trained model\n",
    "def classify_question(question, vectorizer, classifier):\n",
    "    X = vectorizer.transform([question])\n",
    "    \n",
    "    # Classify the customer question\n",
    "    category_id = classifier.predict(X)[0]\n",
    "    category = faq_categories[category_id]\n",
    "    \n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "007d1c24-a42a-45d2-bd8f-38dca12fbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier and entity recognition\n",
    "vectorizer, classifier = train_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "60012ab7-8301-46ba-a3dc-42cf32b1459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello. How can I help you?\n",
      "***Enter bye/end/quit to end the chat***\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  what\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Remove the \"User:\" prefix from the input\u001b[39;00m\n\u001b[0;32m     13\u001b[0m user_input \u001b[38;5;241m=\u001b[39m user_input\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m---> 15\u001b[0m category \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m entities \u001b[38;5;241m=\u001b[39m extract_entities(user_input)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, category)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mclassify_question\u001b[1;34m(question, vectorizer, classifier)\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([question])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Classify the customer question\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m category_id \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m category \u001b[38;5;241m=\u001b[39m faq_categories[category_id]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m category\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:881\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1114\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         )\n\u001b[0;32m   1118\u001b[0m     )\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1126\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1126\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1127\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1128\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:179\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[1;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    178\u001b[0m     )\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2561\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2560\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2561\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2619\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2616\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2619\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2622\u001b[0m     )\n\u001b[0;32m   2624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2628\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "print(\"Chatbot: Hello. How can I help you?\\n***Enter bye/end/quit to end the chat***\") \n",
    "while True:\n",
    "    \n",
    "    # Get user input\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    # Check if user wants to end the conversation\n",
    "    if user_input.lower() in [\"bye\", \"end\", \"quit\"]:\n",
    "        print(\"ChatBot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Remove the \"User:\" prefix from the input\n",
    "    user_input = user_input.replace(\"User: \", \"\").strip()\n",
    "    \n",
    "    category = classify_question(user_input, vectorizer, classifier)\n",
    "    entities = extract_entities(user_input)\n",
    "    \n",
    "    print(\"Category:\", category)\n",
    "    print(\"Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d49c94-2643-47a3-a2ee-87e79fdccf4b",
   "metadata": {},
   "source": [
    "GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b6664aa-8a43-4d6f-99ac-fc07e2520f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e0cf36-62ff-4e83-b84d-90a0f6c16b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8281b0533d4844428a859b34c1df78b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahap\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sahap\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876e49bc67e145e4a59cb1ff73babb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e309a127bb6c4675855ea170adfb6f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download and load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2881b79f-0fa2-4b09-9298-e8433c71423d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b885defbd703473db09928bd15d1398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4f5d95123c4e92bb012dd1a5590585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download and load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aede1585-5f85-4c39-a248-0ee1eac27199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73f56c0f-de3f-4e9b-94a2-12bbdf12205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the generation parameters\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\", config=model_config)\n",
    "model_config.pad_token_id = model_config.eos_token_id\n",
    "\n",
    "# Encode input text\n",
    "input_text = \"Hey how are you? i need some help changing the password to my bank account\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "71e00bfe-6b65-4f1c-968b-7f44ebddf4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output using the model\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    #config=model_config,\n",
    "    num_return_sequences=1,  # Number of different sequences to generate\n",
    "    do_sample=True,  # Enable random sampling\n",
    "    temperature=0.7,  # Controls randomness, higher value -> more random\n",
    "    repetition_penalty=1.0,  # Controls repetition, higher value -> less repetition\n",
    "    length_penalty=1.0,  # Controls output length, lower value -> shorter length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17cccf36-fa32-46c8-8b1a-25b2035accd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey how are you? i need some help changing the password to my bank account now. i need this to be saved. so i can pay for this. i need to get my car back to the dealership. i need the car keys to the dealership.Give me this and i'll pay for the car keys.I can use this to get my car back to the dealership.I need to get my car back to the dealership.\n",
      "\n",
      "Anonymous 02/22/16 (Thu) 07\n"
     ]
    }
   ],
   "source": [
    "# Decode and print the generated response\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970e063-e964-4bc3-b264-6dcdbcfae423",
   "metadata": {},
   "source": [
    "Save GP2 and run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "527433d3-7951-432f-b6b8-27e52f08707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce7f565c-abf9-41e3-baf6-14651f8b0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Download and load the GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Download and load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Download and load the GPT-2 model\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Save the tokenizer and model to your local computer\n",
    "tokenizer.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/tokenizer\")\n",
    "model.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/model\")\n",
    "model_config.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/model_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b8e8a8f-4208-4f1a-a919-f7f40aaeff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer and model from the local saved files\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/tokenizer\")\n",
    "model_config = GPT2Config.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/model_config\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/model\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62e5f1c7-19ab-4c4c-b596-6be8b73fa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the attention mask and pad token ID\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ec4c878-f589-466f-94c1-f11c64fe89a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  i want to open a new account but I am unable to contact my account banker.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot:  summarize: \n",
      "I have been told that the bank will not be able, and they are going through with it because there is no way of getting an answer from them about what happened in this case (i think). So if you would like me on your side please send us some information so we can find out more details as soon possible!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    # Check if user wants to end the conversation\n",
    "    if user_input.lower() in [\"bye\", \"end\", \"quit\"]:\n",
    "        print(\"ChatBot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Remove the \"User:\" prefix from the input\n",
    "    user_input = user_input.replace(\"User: \", \"\").strip()\n",
    "    \n",
    "    # Tokenize the user input\n",
    "    input_ids = tokenizer.encode(\"\"+user_input, return_tensors=\"pt\")\n",
    "\n",
    "    # Set the attention mask\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    # Generate response using the model\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=100,\n",
    "        num_return_sequences=1,  # Number of different sequences to generate\n",
    "        do_sample=True,  # Enable random sampling\n",
    "        temperature=0.2,  # Controls randomness, higher value -> more random 0.7\n",
    "        repetition_penalty=7.0,  # Controls repetition, higher value -> less repetition\n",
    "        length_penalty=0.5,  # Controls output length, lower value -> shorter length\n",
    "    )\n",
    "\n",
    "    # Convert the output ids to text\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Remove the input text from the generated response\n",
    "    response = response.replace(user_input, \"\").strip()\n",
    "\n",
    "    print(\"ChatBot: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65412002-3fb5-44d7-b40d-ab64f1c1c6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69b67698-16f1-431b-9f8e-73df2a7525fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum length for generated responses\n",
    "max_response_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "242d6b8c-1ae0-4bfd-8759-f89fe6957e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customize the generation parameters\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\", config=model_config)\n",
    "model_config.pad_token_id = model_config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d638f2d4-474f-478c-93fa-ded887e1c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hello how are you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatBot: hello how are you today?\n",
      "\n",
      "I'm sorry, but my first love was to my sister.\n",
      "\n",
      "What do you do for a living?\n",
      "\n",
      "I live as a student.\n",
      "\n",
      "My parents died.\n",
      "\n",
      "Did you go to university?\n",
      "\n",
      "Yes.\n",
      "\n",
      "You have an amazing job?rafted in?\n",
      "\n",
      "Yes.\n",
      "\n",
      "You don't work at a big corporate organization?\n",
      "\n",
      "No, I don't work at a big corporate organization.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Tokenize the user input\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(user_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1074\u001b[0m     )\n\u001b[1;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # Tokenize the user input\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response using the model\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_response_length,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "    )\n",
    "\n",
    "    # Convert the output ids to text\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"ChatBot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af4d762a-de19-4efb-9e0c-8bf8a58f08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c1c91fe0-2f6d-4c24-8fb8-dc513e9bd425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d09e558f3d483dbd27ebfa51d1c901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf033102706422dad1f3c0db305cb2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf2f4ab122b4c5fa78508b67e761d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1176274bc533402799a3284a63d13794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996980428695679}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51db8e05-4a6e-49e4-a5d9-cfef7a3dec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9994600415229797}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier('This is ridiculous. This is the fourth time of me trying to call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986adab-601c-43b1-9c47-ecfdbd85f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Save DistillBert and run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "215cad66-9093-4776-b204-27819ff26cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4bcea4d0-b945-443c-a448-11f16c48e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'  # or other DistilBERT variants\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "classification_model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5aa3e7ba-a1a9-4560-a9e0-d796df9f73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and model to your local computer\n",
    "tokenizer.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/tokenizer\")\n",
    "classification_model.save_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d142a21a-80fc-4685-9880-3b7cf94debe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model from the local saved files\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/tokenizer\")\n",
    "classification_model = DistilBertForSequenceClassification.from_pretrained(\"C:/Users/sahap/Documents/Chatbot/DistilBert/model\", num_labels=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ef6b5bbc-0216-461e-a7ef-77cb24d9d99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8972105464272068\n",
      "Recall: 0.8972105464272068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('faq_data.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['question'], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the SVM classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ed4617f-8753-4e10-954e-4c5b70ff48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1}\n",
      "Accuracy: 0.8972105464272068\n",
      "Recall: 0.8972105464272068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Create a parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "\n",
    "# Create a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best classifier\n",
    "predictions = best_classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ae99d39-c6eb-4d8e-9b6d-2411465e42d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Perform grid search with cross-validation\u001b[39;00m\n\u001b[0;32m     25\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(classifier, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters and classifier\u001b[39;00m\n\u001b[0;32m     29\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 255\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:355\u001b[0m, in \u001b[0;36mBaseLibSVM._sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    342\u001b[0m kernel_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_kernels\u001b[38;5;241m.\u001b[39mindex(kernel)\n\u001b[0;32m    344\u001b[0m libsvm_sparse\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    346\u001b[0m (\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    349\u001b[0m     dual_coef_data,\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m--> 355\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm_sparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibsvm_sparse_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32msklearn\\svm\\_libsvm_sparse.pyx:191\u001b[0m, in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py:27\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_cs_matrix\u001b[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124;03m\"\"\"base matrix class for compressed row- and column-oriented matrices\"\"\"\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg1, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m         _data_matrix\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m isspmatrix(arg1):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('faq_data.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['question'], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10]}\n",
    "\n",
    "# Create an SVM classifier\n",
    "classifier = SVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on user input\n",
    "while True:\n",
    "    user_input = input(\"Enter your question (or 'q' to quit): \")\n",
    "    if user_input == 'q':\n",
    "        break\n",
    "\n",
    "    # Transform user input into vector using the same vectorizer\n",
    "    user_input_vector = vectorizer.transform([user_input])\n",
    "\n",
    "    # Predict the category of the user input\n",
    "    prediction = best_classifier.predict(user_input_vector)[0]\n",
    "\n",
    "    # Frame the category as a sentence\n",
    "    category_sentence = f\"The category of your question is: {prediction}\"\n",
    "\n",
    "    print(category_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437a499-63ee-4820-8c63-a1c0300821f5",
   "metadata": {},
   "source": [
    "################# FINAl CATEGORY AND NER ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1f875-1623-47a4-9fac-bf11e630085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('faq_data.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['question'], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10]}\n",
    "\n",
    "# Create a LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Get the best hyperparameters and classifier\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Load Stanford NER model\n",
    "stanford_ner_model = 'path_to_stanford_ner_model'\n",
    "stanford_ner_jar = 'path_to_stanford_ner_jar'\n",
    "\n",
    "# Create NER tagger\n",
    "ner_tagger = StanfordNERTagger(stanford_ner_model, stanford_ner_jar, encoding='utf-8')\n",
    "\n",
    "# Function to extract entities from the question using NER\n",
    "def extract_entities(question):\n",
    "    tokenized_question = word_tokenize(question)\n",
    "    entities = ner_tagger.tag(tokenized_question)\n",
    "    return [entity for entity, tag in entities if tag != 'O']\n",
    "\n",
    "# Function to classify user input and frame the response\n",
    "def classify_question(user_input):\n",
    "    user_input_vector = vectorizer.transform([user_input])\n",
    "    predicted_category = best_classifier.predict(user_input_vector)[0]\n",
    "    entities = extract_entities(user_input)\n",
    "    response = f\"The category of your question is {predicted_category}. The entities mentioned are: {', '.join(entities)}.\"\n",
    "    return response\n",
    "\n",
    "# Chatbot loop\n",
    "while True:\n",
    "    user_input = input(\"Please enter your question (or 'q' to quit): \")\n",
    "    if user_input.lower() == 'q':\n",
    "        break\n",
    "    else:\n",
    "        response = classify_question(user_input)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "54036979-b314-4abb-829c-35e16b930549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\sahap\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5c7fbbca-b2f4-424e-9f67-5d71cb7a5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e1986787-5c32-44d2-b014-4af931927ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [109]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9c6266c-b735-4883-ae4a-19d24a3db7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    summarized_text = summarize(text)\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7334c997-3126-4f07-b644-8490ebf1510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from sumy) (2.27.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from sumy) (3.7)\n",
      "Collecting pycountry>=18.2.23\n",
      "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting breadability>=0.1.20\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "Collecting docopt<0.7,>=0.6.1\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: chardet in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.8.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.64.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from pycountry>=18.2.23->sumy) (61.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\sahap\\anaconda3\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Building wheels for collected packages: breadability, docopt, pycountry\n",
      "  Building wheel for breadability (setup.py): started\n",
      "  Building wheel for breadability (setup.py): finished with status 'done'\n",
      "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21712 sha256=6ec75ca595f7539391cd6be1824e45bde82f393b74023b81900988bb2e0a3119\n",
      "  Stored in directory: c:\\users\\sahap\\appdata\\local\\pip\\cache\\wheels\\ba\\9f\\70\\7795228568b81b57a8932755938da9fb1f291b0576752604aa\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=241009cc79ee65a8b0313783ebc17cc43377ce5ea2b2dc924a4e76dce2c69b46\n",
      "  Stored in directory: c:\\users\\sahap\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for pycountry (PEP 517): started\n",
      "  Building wheel for pycountry (PEP 517): finished with status 'done'\n",
      "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681896 sha256=ba56da6bb0a3a45842a523c309435af1f9742e6931ed60f2602c7bd0320df79c\n",
      "  Stored in directory: c:\\users\\sahap\\appdata\\local\\pip\\cache\\wheels\\47\\15\\92\\e6dc85fcb0686c82e1edbcfdf80cfe4808c058813fed0baa8f\n",
      "Successfully built breadability docopt pycountry\n",
      "Installing collected packages: docopt, pycountry, breadability, sumy\n",
      "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7f774691-97e1-461b-8635-11309791d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f55805d4-309f-44d6-8bd7-f3a4414fed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_words):\n",
    "    # Initialize TextRankSummarizer\n",
    "    summarizer = TextRankSummarizer()\n",
    "\n",
    "    # Parse the text\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "\n",
    "    # Summarize the text\n",
    "    summary_sentences = summarizer(parser.document, len(parser.document.sentences))\n",
    "    \n",
    "    # Combine the summary sentences into a single string\n",
    "    summary = ' '.join(str(sentence) for sentence in summary_sentences)\n",
    "    \n",
    "    # Truncate the summary to the specified number of words\n",
    "    summary_words = summary.split()[:max_words]\n",
    "    summary = ' '.join(summary_words)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f56c51c4-c660-40bd-92bf-7e6cf7ab01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The text to be summarized\n",
    "text = \"Many prepaid debit cards let you access important banking functions, such as direct deposit, online bill pay and electronic bank transfers to savings accounts (which can help you build your savings balance). They are generally available to customers regardless of banking history. Some of the best choices have no or low monthly fees and access to thousands of ATMs. Not being able to open a bank account shouldn’t mean not being able to access banking. Whether you resolve the account issue directly with the institution or choose to shop around, there are a number of solid financial service companies that would be happy to have your business.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e255e542-ec66-4ac6-b1f0-674051ce2ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Many prepaid debit cards let you\n"
     ]
    }
   ],
   "source": [
    "max_words = 6\n",
    "\n",
    "# Generate the summary\n",
    "summary = summarize_text(text, max_words)\n",
    "\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bd1facc4-67d7-4005-a464-52e80e8c382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The apples were red and the mangoes were green.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "def summarize_text(text, num_sentences=1):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summarized_sentences = summarizer(parser.document, num_sentences)\n",
    "    summarized_text = ' '.join(str(sentence) for sentence in summarized_sentences)\n",
    "    return summarized_text\n",
    "\n",
    "text = \"Alfred went to the city. He bought two mangoes and 5 apples. The apples were red and the mangoes were green. He spent 6 hours in the city.\"\n",
    "summary = summarize_text(text, num_sentences=1)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfbcd72-ae67-4542-baf8-ad378149ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: He spent 6 hours in the city buying mangoes and apples.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "def summarize_text(text, num_sentences=1):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summarized_sentences = summarizer(parser.document, num_sentences)\n",
    "    summarized_text = ' '.join(str(sentence) for sentence in summarized_sentences)\n",
    "    return summarized_text\n",
    "\n",
    "text = \"Alfred went to the city. He bought two mangoes and 5 apples. The mangoes were red and the apples were green. He spent 6 hours in the city buying mangoes and apples. \"\n",
    "summary = summarize_text(text, num_sentences=1)\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4b4f90-3d14-4804-adfd-e3e196dbffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def summarize_text(text, num_sentences=1):\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs, num_beams=4, max_length=100, early_stopping=True)\n",
    "\n",
    "    # Decode the summary and limit the number of sentences\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summary_sentences = summary.split('. ')[:num_sentences]\n",
    "    summary = '. '.join(summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95c7e5ad-5639-44fb-9b2f-78c56c4a6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital. Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well. Therefore, Peter stayed with her at the hospital for 3 days without leaving.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad28a2b-1514-495b-a17f-5128a440700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Peter and Elizabeth went to a night party in the city. while there, Elizabeth collapsed and was rushed to the hospital. the doctor told Peter to stay besides her until she gets well.\n"
     ]
    }
   ],
   "source": [
    "#text = \"Alfred went to the city to buy ingredients for a dessert. He bought two mangoes and 5 apples. The mangoes were red and the apples were green. He spent 6 hours in the city buying mangoes and apples. He bought the red mangoes and green apples to make a dessert.\"\n",
    "summary = summarize_text(text, num_sentences=3)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0410a51d-41bc-437e-abb8-4752d218641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416a50e-1e74-4bf1-b7e2-39008260cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from fuzzywuzzy import fuzz\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309e040-5471-4480-9226-f370e00d2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Stanford NER\n",
    "stanford_ner_jar = '/path/to/stanford-ner.jar'\n",
    "stanford_ner_model = '/path/to/stanford-ner-model.ser.gz'\n",
    "\n",
    "# Load Stanford NER Tagger\n",
    "st = StanfordNERTagger(stanford_ner_model, stanford_ner_jar, encoding='utf-8')\n",
    "\n",
    "# Define regex patterns for category identification\n",
    "payment_status_pattern = r'\\b(payment\\s*(status|status\\s*for))\\b'\n",
    "payment_history_pattern = r'\\b(payment\\s*(history|history\\s*of))\\b'\n",
    "balance_pattern = r'\\b(balance|available\\s*balance)\\b'\n",
    "transaction_pattern = r'\\b(transaction|transaction\\s*history)\\b'\n",
    "\n",
    "# Define regex pattern for entity extraction\n",
    "entity_pattern = r'(?i)\\b(for|of|about|regarding)\\s+(\\w+)\\b'\n",
    "\n",
    "# Function to classify user input into categories\n",
    "def classify_category(user_input):\n",
    "    user_input = user_input.lower()  # Convert input to lowercase for better comparison\n",
    "    if re.search(payment_status_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Payment Status'\n",
    "    elif re.search(payment_history_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Payment History'\n",
    "    elif re.search(balance_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Balance'\n",
    "    elif re.search(transaction_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Transaction'\n",
    "    else:\n",
    "        # Fuzzy matching to handle spelling mistakes\n",
    "        status_score = fuzz.partial_ratio(user_input, 'payment status')\n",
    "        history_score = fuzz.partial_ratio(user_input, 'payment history')\n",
    "        balance_score = fuzz.partial_ratio(user_input, 'balance')\n",
    "        transaction_score = fuzz.partial_ratio(user_input, 'transaction')\n",
    "        if status_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Payment Status'\n",
    "        elif history_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Payment History'\n",
    "        elif balance_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Balance'\n",
    "        elif transaction_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Transaction'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "# Function to extract entities using Stanford NER\n",
    "def extract_entities(user_input):\n",
    "    tokens = word_tokenize(user_input)\n",
    "    tagged_entities = st.tag(tokens)\n",
    "    entities = []\n",
    "    for entity, tag in tagged_entities:\n",
    "        if tag != 'O':\n",
    "            entities.append((entity, tag))\n",
    "    return entities\n",
    "\n",
    "# Function to send data to SQL microservice and get results\n",
    "def query_sql_microservice(category, entities):\n",
    "    # Prepare the data payload to send to the microservice\n",
    "    payload = {\n",
    "        'category': category,\n",
    "        'entities': entities\n",
    "    }\n",
    "\n",
    "    # Send a POST request to the SQL microservice API endpoint\n",
    "    response = requests.post('https://your-sql-microservice-api.com/query', json=payload)\n",
    "\n",
    "    # Check the response status code\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # Check if the user wants to end the chat\n",
    "    if user_input.lower() in ['quit', 'bye', 'end']:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Classify user input into categories\n",
    "    category = classify_category(user_input)\n",
    "    print(f\"Bot: Category: {category}\")\n",
    "\n",
    "    # Extract entities from user input using Stanford NER\n",
    "    entities = extract_entities(user_input)\n",
    "    if entities:\n",
    "        print(\"Bot: Entities:\")\n",
    "        for entity, entity_type in entities:\n",
    "            print(f\"  - Entity: {entity}, Type: {entity_type}\")\n",
    "\n",
    "        # Send data to SQL microservice and retrieve results\n",
    "        results = query_sql_microservice(category, entities)\n",
    "        if results:\n",
    "            print(\"Bot: SQL Microservice Results:\")\n",
    "            for result in results:\n",
    "                print(result)\n",
    "        else:\n",
    "            print(\"Bot: Error retrieving SQL microservice results.\")\n",
    "    else:\n",
    "        print(\"Bot: No entities found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9c32e-57d8-4dad-bbcb-1092ab93de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from fuzzywuzzy import fuzz\n",
    "import csv\n",
    "\n",
    "# Configure Stanford NER\n",
    "stanford_ner_jar = '/path/to/stanford-ner.jar'\n",
    "stanford_ner_model = '/path/to/stanford-ner-model.ser.gz'\n",
    "\n",
    "# Load Stanford NER Tagger\n",
    "st = StanfordNERTagger(stanford_ner_model, stanford_ner_jar, encoding='utf-8')\n",
    "\n",
    "# Define regex patterns for category identification\n",
    "payment_status_pattern = r'\\b(payment\\s*(status|status\\s*for))\\b'\n",
    "payment_history_pattern = r'\\b(payment\\s*(history|history\\s*of))\\b'\n",
    "balance_pattern = r'\\b(balance|available\\s*balance)\\b'\n",
    "transaction_pattern = r'\\b(transaction|transaction\\s*history)\\b'\n",
    "\n",
    "# Define regex pattern for entity extraction\n",
    "entity_pattern = r'(?i)\\b(for|of|about|regarding)\\s+(\\w+)\\b'\n",
    "\n",
    "# Function to classify user input into categories\n",
    "def classify_category(user_input):\n",
    "    user_input = user_input.lower()  # Convert input to lowercase for better comparison\n",
    "    if re.search(payment_status_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Payment Status'\n",
    "    elif re.search(payment_history_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Payment History'\n",
    "    elif re.search(balance_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Balance'\n",
    "    elif re.search(transaction_pattern, user_input, re.IGNORECASE):\n",
    "        return 'Transaction'\n",
    "    else:\n",
    "        # Fuzzy matching to handle spelling mistakes\n",
    "        status_score = fuzz.partial_ratio(user_input, 'payment status')\n",
    "        history_score = fuzz.partial_ratio(user_input, 'payment history')\n",
    "        balance_score = fuzz.partial_ratio(user_input, 'balance')\n",
    "        transaction_score = fuzz.partial_ratio(user_input, 'transaction')\n",
    "        if status_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Payment Status'\n",
    "        elif history_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Payment History'\n",
    "        elif balance_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Balance'\n",
    "        elif transaction_score > 80:  # Adjust the threshold as per your needs\n",
    "            return 'Transaction'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "# Function to extract entities using Stanford NER\n",
    "def extract_entities(user_input):\n",
    "    tokens = word_tokenize(user_input)\n",
    "    tagged_entities = st.tag(tokens)\n",
    "    entities = []\n",
    "    for entity, tag in tagged_entities:\n",
    "        if tag != 'O':\n",
    "            entities.append((entity, tag))\n",
    "    return entities\n",
    "\n",
    "# Function to query the CSV file based on category and entities\n",
    "def query_csv(category, entities):\n",
    "    # Load the CSV file\n",
    "    with open('payments_data.csv', 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        data = list(reader)\n",
    "\n",
    "    if category == 'Payment Status':\n",
    "        entity = entities[0][0] if entities else None\n",
    "        if entity:\n",
    "            # Filter the data based on entity for payment status\n",
    "            filtered_data = [row for row in data if row['Entity'] == entity]\n",
    "            return filtered_data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    elif category == 'Payment History':\n",
    "        entity = entities[0][0] if entities else None\n",
    "        date_entities = [entity[0] for entity in entities if entity[1] == 'DATE']\n",
    "        if entity and len(date_entities) >= 2:\n",
    "            # Filter the data based on entity and date range for payment history\n",
    "            date1 = date_entities[0]\n",
    "            date2 = date_entities[1]\n",
    "            filtered_data = [row for row in data if row['Entity'] == entity and row['Date'] >= date1 and row['Date'] <= date2]\n",
    "            return filtered_data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Add additional conditions for other categories (Balance, Transaction) if needed\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    # Check if the user wants to end the chat\n",
    "    if user_input.lower() in ['quit', 'bye', 'end']:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Classify user input into categories\n",
    "    category = classify_category(user_input)\n",
    "    print(f\"Bot: Category: {category}\")\n",
    "\n",
    "    # Extract entities from user input using Stanford NER\n",
    "    entities = extract_entities(user_input)\n",
    "    if entities:\n",
    "        print(\"Bot: Entities:\")\n",
    "        for entity, entity_type in entities:\n",
    "            print(f\"  - Entity: {entity}, Type: {entity_type}\")\n",
    "\n",
    "        # Query the CSV file and retrieve results\n",
    "        results = query_csv(category, entities)\n",
    "        if results:\n",
    "            print(\"Bot: Results from 'payments_data.csv':\")\n",
    "            for result in results:\n",
    "                print(result)\n",
    "        else:\n",
    "            print(\"Bot: No results found in 'payments_data.csv' for the given category and entities.\")\n",
    "    else:\n",
    "        print(\"Bot: No entities found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
